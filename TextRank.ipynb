{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":32267,"sourceType":"datasetVersion","datasetId":24984}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading data into dataframe for ease of access","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport pandas as pd\npath_, filename_, category_, article_or_summary_ = [],[],[],[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path_.append(os.path.join(dirname, filename))\n        filename_.append(filename)\n        category_.append(dirname.split(\"/\")[-1])\n        article_or_summary_.append(dirname.split(\"/\")[-2])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T21:52:00.961492Z","iopub.execute_input":"2024-06-04T21:52:00.963066Z","iopub.status.idle":"2024-06-04T21:52:01.069257Z","shell.execute_reply.started":"2024-06-04T21:52:00.963004Z","shell.execute_reply":"2024-06-04T21:52:01.067871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\"path\":path_, \"filename\":filename_, \"category\":category_, \"article_or_summary\":article_or_summary_}, columns=[\"path\", \"filename\", \"category\", \"article_or_summary\"])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-04T21:52:01.071712Z","iopub.execute_input":"2024-06-04T21:52:01.072140Z","iopub.status.idle":"2024-06-04T21:52:01.101258Z","shell.execute_reply.started":"2024-06-04T21:52:01.072103Z","shell.execute_reply":"2024-06-04T21:52:01.099917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id='6'></a>\n\n# <p style=\"padding:10px;background-color:#6600cc ;margin:10;color:#ccff99;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Sentence Tokenization</p>","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nimport numpy as np\nimport networkx as nx\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-06-04T21:52:01.102809Z","iopub.execute_input":"2024-06-04T21:52:01.103186Z","iopub.status.idle":"2024-06-04T21:52:01.109138Z","shell.execute_reply.started":"2024-06-04T21:52:01.103152Z","shell.execute_reply":"2024-06-04T21:52:01.107787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_tokenizer(text):        \n    sentences =[]        \n    sentences = sent_tokenize(text)    \n    for sentence in sentences:        \n        sentence.replace(\"[^a-zA-Z0-9]\",\" \")     \n    return sentences\n\ndef read_articles_and_summary(df, category):\n    \"\"\"\n    read all the articles in category\n    \"\"\"\n    articles = {}\n    summaries = {}\n    \n    df = df[df['category']==category]\n    for _, row in df.iterrows():\n        file_path = row['path']\n        file_name = row['filename']\n        file_type = row['article_or_summary']\n        \n        with open(file_path, \"r\") as f:\n            content = f.read()\n            if file_type == 'News Articles':\n                articles[(category, file_name)] = content\n            elif file_type == \"Summaries\":\n                summaries[(category, file_name)] = content\n    return articles, summaries\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-04T21:52:01.110866Z","iopub.execute_input":"2024-06-04T21:52:01.111333Z","iopub.status.idle":"2024-06-04T21:52:01.123539Z","shell.execute_reply.started":"2024-06-04T21:52:01.111287Z","shell.execute_reply":"2024-06-04T21:52:01.122197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spell Correction ","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nimport tensorflow_hub as hub\n\n\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\n\ndef spell_correction(sent_tok):\n    mod_sent = []\n    for tok in sent_tok:\n        blob_obj = TextBlob(tok)\n        correct_sent = str(blob_obj.correct())\n#         print(f\"\\033[94m Original Token : {tok} \\033[0m\")\n#         print(f\"\\033[92m Corrected Token: {correct_sent} \\033[92m\")\n        mod_sent.append(correct_sent)\n    return \" \".join(mod_sent)\n\ndef sentence_similarity(sent1,sent2,embed):  \n    A = embed([sent1])[0]\n    B = embed([sent2])[0]\n    return 1 - (np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B)))\n\ndef build_similarity_matrix(sentences,embeds):\n    similarity_matrix = np.zeros((len(sentences),len(sentences)))\n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1!=idx2:\n                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2],embeds)\n    return similarity_matrix\n\n\n# sim_mat = build_similarity_matrix(mod_sent, embed)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-04T21:52:01.126732Z","iopub.execute_input":"2024-06-04T21:52:01.127194Z","iopub.status.idle":"2024-06-04T21:52:08.281428Z","shell.execute_reply.started":"2024-06-04T21:52:01.127155Z","shell.execute_reply":"2024-06-04T21:52:08.280040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-06-04T21:52:08.283471Z","iopub.execute_input":"2024-06-04T21:52:08.283991Z","iopub.status.idle":"2024-06-04T21:52:19.653234Z","shell.execute_reply.started":"2024-06-04T21:52:08.283941Z","shell.execute_reply":"2024-06-04T21:52:19.651467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n\n# construct a dataframe\n\n# 函数来计算ROUGE分数\ndef compute_rouge_scores(generate, reference):\n    rouge = Rouge()\n    scores = rouge.get_scores(generate, reference, avg=True)\n    return pd.Series({\n        'rouge1_precision': scores['rouge-1']['p'],\n        'rouge1_recall': scores['rouge-1']['r'],\n        'rouge1_fmeasure': scores['rouge-1']['f'],\n        'rouge2_precision': scores['rouge-2']['p'],\n        'rouge2_recall': scores['rouge-2']['r'],\n        'rouge2_fmeasure': scores['rouge-2']['f'],\n        'rougeL_precision': scores['rouge-l']['p'],\n        'rougeL_recall': scores['rouge-l']['r'],\n        'rougeL_fmeasure': scores['rouge-l']['f']\n    })\n\n\ndef generate_single_summary(text,top_n,embeds):\n    summarize_text = []  \n    sentences = sent_tokenizer(text)           \n    sentence_similarity_matrix = build_similarity_matrix(sentences,embeds)  \n    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n    scores = nx.pagerank(sentence_similarity_graph) \n    \n    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n    top_n = min(len(ranked_sentences), top_n)\n    \n    for i in range(top_n):\n        summarize_text.append(ranked_sentences[i][1]) \n    return \" \".join(summarize_text)\n\n\n# def generate_summaries(df, category):\n#     generated_sumaries = {}\n#     rouge_scores = {}\n#     # get articles and summaries\n#     articles, summaries = read_articles_and_summary(df, category)\n#     print(\"Articles and Summaries are Loaded\")\n    \n#     for (category, file_name), text in articles.items():\n#         print(f\"Category: {category}, Article: {file_name}\")\n#         sent_tok = sent_tokenizer(text)\n#         original_text = spell_correction(sent_tok)\n#         summarized_text = generate_single_summary(original_text, top_n=5, embeds=embed)\n#         generated_sumaries[(category, file_name)] = summarized_text\n        \n#         if (category, file_name) in summaries:\n#             article_summaries = summaries[(category, file_name)]\n#             rouge_score = compute_rouge(summarized_text, article_summaries)\n            \n#     return generated_summaries, rouge_scores\n\n#     for (category, article_name), scores in rouge_scores.items():\n#         print(f\"Category: {category}, Article: {article_name}\")\n#         for idx, score in enumerate(scores):\n#             print(f\"  Summary {idx+1}: {score}\")\n\n# 生成摘要并计算ROUGE分数的函数\ndef generate_summaries(df, category, embed, top_n):\n    data = []\n    articles, summaries = read_articles_and_summary(df, category)\n    print(\"Articles and Summaries are Loaded\")\n    \n    for (category, file_name), text in articles.items():\n        print(f\"Category: {category}, Article: {file_name}\")\n        sent_tok = sent_tokenizer(text)\n        original_text = spell_correction(sent_tok)\n        summarized_text = generate_single_summary(original_text, top_n, embed)\n        \n        if (category, file_name) in summaries:\n            reference_summary = summaries[(category, file_name)]\n            rouge_scores = compute_rouge_scores(summarized_text, reference_summary)\n            data.append({\n                'actual_summary': reference_summary,\n                'generated_summary': summarized_text,\n                **rouge_scores\n            })\n    \n    return pd.DataFrame(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T00:10:29.456244Z","iopub.execute_input":"2024-06-05T00:10:29.456721Z","iopub.status.idle":"2024-06-05T00:10:29.472541Z","shell.execute_reply.started":"2024-06-05T00:10:29.456680Z","shell.execute_reply":"2024-06-05T00:10:29.471284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = generate_summaries(df, \"politics\", embed, top_n=5)\nresult_df.to_pandas(\"/kaggle/working/result.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T00:10:41.972747Z","iopub.execute_input":"2024-06-05T00:10:41.973165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining Function of Summary which is basically combined actions which we have gone through in the above steps.\nTo be more clear we will be collection Nth top most relevant sentences to summarize entire articles.\n\nSteps:\n\n1. Reading Article and extracting Text from it.\n2. Generate Sentence tokens.\n3. Compute cosine similarity.\n4. Using NetworkX to compute Graph Similiarity nodes.\n5. Using Page Ranking method to rank the sentences.\n6. Collect Top N Sentences and represent as summary of the Entire Article.\n\nNote : The Above steps metioned is applicaple for Extractive Strategy for Text Summarization \n","metadata":{}}]}